# Fast Algorithms for Mining Association Rules

## 3 Performance

### 3.3 Generation of Synthetic Data

We generated synthetic transactions to evaluate the performance of the algorithms over a large range of data characteristics.
These transactions mimic the transactions in the retailing environment.
Our model of the "real" world is that people tend to buy sets of items together.
Each such set is potentially a maximal large itemset.
An example of such a set might be sheets, pillow case, comforter, and ruffles.
However, some people may buy only some of the items from such a set.
For instance, some people might buy only sheets and pillow case, and some only sheets.
A transaction may contain more than one large itemset.
For example, a customer might place an order for a dress and jacket when ordering sheets and pillow cases, where the dress and jacket together form another large itemset.
Transaction sizes are typically clustered around a mean and a few transactions have many items.
Typical sizes of large itemsets are also clustered around a mean, with a few large itemsets having a large number of items.
To create a dataset, our synthetic data generation program takes the parameters shown in Table 2.

Table 2: Parameters
|       |                                                        |
| :---: | :----------------------------------------------------: |
| \|D\| | Number of transactions                                 |
| \|T\| | Average size of the transactions                       |
| \|I\| | Average size of the maximal potentially large itemsets |
| \|L\| | Number of maximal potentially large itemsets           |
|   N   | Number of items                                        |

We first determine the size of the next transaction.
The size is picked from a Poisson distribution with mean μ equal to |T|.
Note that if each item is chosen with the same probability p, and there are N items,
the expected number of items in a transaction is given by a binomial distribution with parameters N and p,
and is approximated by a Poisson distribution with mean N p.

We then assign items to the transaction.
Each transaction is assigned a series of potentially large itemsets.
If the large itemset on hand does not fit in the transaction,
the itemset is put in the transaction anyway in half the cases,
and the itemset is moved to the next transaction the rest of the cases.

Large itemsets are chosen from a set T of such itemsets.
The number of itemsets in T is set to |L|.
There is an inverse relationship between |L| and the average support for potentially large itemsets.
An itemset in T is generated by first picking the size of the itemset from a Poisson distribution with mean μ equal to |I|.
Items in the first itemset are chosen randomly.
To model the phenomenon that large itemsets often have common items,
some fraction of items in subsequent itemsets are chosen from the previous itemset generated.
We use an exponentially distributed random variable with mean equal to the correlation level to decide this fraction for each itemset.
The remaining items are picked at random.
In the datasets used in the experiments, the correlation level was set to 0.5.
We ran some experiments with the correlation level set to 0.25 and 0.75 but did not find much difference in the nature of our performance results.

Each itemset in T has a weight associated with it, which corresponds to the probability that this itemset will be picked.
This weight is picked from an exponential distribution with unit mean, and is then normalized so that the sum of the weights for all the itemsets in T is 1.
The next itemset to be put in the transaction is chosen from T by tossing an |L| sided weighted coin, where the weight for a side is the probability of picking the associated itemset.

To model the phenomenon that all the items in a large itemset are not always bought together,
we assign each itemset in T a corruption level c.
When adding an itemset to a transaction,
we keep dropping an item from the itemset as long as a uniformly distributed random number between 0 and 1 is less than c.
Thus for an itemset of size l, we will add l items to the transaction 1-c of the time, l-1 items c(1-c) of the time, l - 2 items c^2(1-c) of the time, etc.
The corruption level for an itemset is fixed and is obtained from a normal distribution with mean 0.5 and variance 0.1.

We generated datasets by setting N = 1000 and |L| = 2000.
We chose 3 values for |T|: 5, 10, and 20.
We also chose 3 values for |I|: 2, 4, and 6.
The number of transactions was to set to 100,000 because, as we will see in Section 3.4, SETM could not be run for larger values.
However, for our scale-up experiments, we generated datasets with up to 10 million transactions (838MB for T20).
Table 3 summarizes the dataset parameter settings.
For the same |T| and |D| values, the size of datasets in megabytes were roughly equal for the different values of |I|.

Table 3: Parameter settings
| Name         | \|T\| | \|I\| | \|D\| | Size in Megabytes |
| :----------: | :---: | :---: | :---: | :---------------: |
| TS.12.DlO0K  |  5    |  2    |  lO0K | 2.4               |
| Tl0.12.DlO0K |  10   |  2    |  lO0K | 4.4               |
| Tl0.14.DlO0K |  10   |  4    |  lO0K |                   |
| T20.12.DlO0K |  20   |  2    |  lO0K | 8.4               |
| T20.14.Dl00K |  20   |  4    |  lO0K |                   |
| T20.16.DlO0K |  20   |  6    |  lOOK |                   |
